# deployment.py
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

class SemiconductorQualityControl:
    """Classe pour d√©ployer et utiliser le mod√®le de contr√¥le qualit√©"""
    
    def __init__(self, model_path=None, metrics_path=None):
        """
        Initialisation du syst√®me de contr√¥le qualit√©
        
        Args:
            model_path: Chemin vers le mod√®le sauvegard√©
            metrics_path: Chemin vers les m√©triques sauvegard√©es
        """
        print("üîß Initialisation du syst√®me de contr√¥le qualit√©...")
        
        # Gestion intelligente des chemins par d√©faut
        if model_path is None:
            # On teste les deux localisations classiques (depuis notebooks/ ou depuis racine)
            paths_to_test = ['results/final_model_smote_rf.pkl', '../results/final_model_smote_rf.pkl']
            model_path = next((p for p in paths_to_test if os.path.exists(p)), paths_to_test[0])
            
        if metrics_path is None:
            paths_to_test = ['results/final_metrics.pkl', '../results/final_metrics.pkl']
            metrics_path = next((p for p in paths_to_test if os.path.exists(p)), paths_to_test[0])
        
        # Charger le mod√®le
        if os.path.exists(model_path):
            self.model = joblib.load(model_path)
            print(f"‚úÖ Mod√®le charg√© depuis : {model_path}")
        else:
            print(f"‚ö†Ô∏è  Mod√®le non trouv√© : {model_path}")
            print("   Cr√©ation d'un mod√®le fictif pour le test...")
            self.model = None
        
        # Charger les m√©triques de r√©f√©rence
        if os.path.exists(metrics_path):
            self.metrics = joblib.load(metrics_path)
            print(f"‚úÖ M√©triques de r√©f√©rence charg√©es depuis : {metrics_path}")
        else:
            self.metrics = None
            print(f"‚ö†Ô∏è  M√©triques de r√©f√©rence non trouv√©es √† : {metrics_path}")
    
    def prepare_data(self, new_data):
        """
        Pr√©pare les nouvelles donn√©es pour la pr√©diction
        
        Args:
            new_data: DataFrame pandas avec les nouvelles donn√©es
        
        Returns:
            DataFrame pr√©par√©
        """
        print("üìä Pr√©paration des donn√©es...")
        
        # V√©rifier que les donn√©es ont la bonne forme
        expected_features = 567  # Nombre de features attendues
        if new_data.shape[1] != expected_features:
            print(f"‚ö†Ô∏è  Attention: {new_data.shape[1]} features au lieu de {expected_features}")
        
        # V√©rifier les valeurs manquantes
        missing_values = new_data.isnull().sum().sum()
        if missing_values > 0:
            print(f"‚ö†Ô∏è  {missing_values} valeurs manquantes d√©tect√©es")
            # Imputation simple (m√©diane)
            imputer = SimpleImputer(strategy='median')
            new_data = pd.DataFrame(imputer.fit_transform(new_data), 
                                   columns=new_data.columns)
            print("‚úÖ Valeurs manquantes imput√©es")
        
        return new_data
    
    def predict(self, new_data, threshold=None):
        """
        Fait des pr√©dictions sur de nouvelles donn√©es
        
        Args:
            new_data: DataFrame pandas
            threshold: Seuil de d√©cision personnalis√© (optionnel)
        
        Returns:
            predictions: Pr√©dictions (0: OK, 1: D√©fectueux)
            probabilities: Probabilit√©s de la classe 1
        """
        print("ü§ñ Pr√©diction en cours...")
        
        if self.model is None:
            print("‚ùå Aucun mod√®le disponible pour la pr√©diction")
            return None, None
        
        # Pr√©parer les donn√©es
        prepared_data = self.prepare_data(new_data)
        
        # Faire des pr√©dictions
        try:
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(prepared_data)[:, 1]
                
                # Appliquer un seuil personnalis√© si sp√©cifi√©
                if threshold is not None:
                    predictions = (probabilities >= threshold).astype(int)
                else:
                    predictions = self.model.predict(prepared_data)
            else:
                predictions = self.model.predict(prepared_data)
                probabilities = None
            
            print(f"‚úÖ {len(predictions)} pr√©dictions effectu√©es")
            
            # Statistiques des pr√©dictions
            defect_count = np.sum(predictions == 1)
            ok_count = np.sum(predictions == 0)
            if len(predictions) > 0:
                defect_rate = defect_count / len(predictions) * 100
            else:
                defect_rate = 0
            
            print(f"üìä R√©sum√©: {defect_count} d√©fectueux ({defect_rate:.1f}%), {ok_count} OK")
            
            return predictions, probabilities
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la pr√©diction: {e}")
            return None, None
    
    def evaluate_performance(self, X_test, y_true):
        """
        √âvalue la performance du mod√®le sur un jeu de test
        
        Args:
            X_test: Features de test
            y_true: Labels r√©els
        
        Returns:
            metrics_dict: Dictionnaire des m√©triques
        """
        print("üìà √âvaluation des performances...")
        
        if self.model is None:
            print("‚ùå Aucun mod√®le disponible pour l'√©valuation")
            return None
        
        # Faire des pr√©dictions
        y_pred, _ = self.predict(X_test)
        
        if y_pred is None:
            return None
        
        # Calculer les m√©triques
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)
        recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)
        f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=0)
        cm = confusion_matrix(y_true, y_pred)
        
        # Afficher les r√©sultats
        print("\n" + "="*60)
        print("üìä PERFORMANCES DU MOD√àLE")
        print("="*60)
        print(f"Accuracy:    {accuracy:.4f}")
        print(f"Pr√©cision:   {precision:.4f}")
        print(f"Recall:      {recall:.4f}")
        print(f"F1-score:    {f1:.4f}")
        
        print(f"\nüìã Matrice de confusion:")
        print(f"    | Pr√©dit OK | Pr√©dit D√©fectueux |")
        print(f"    |-----------|-------------------|")
        print(f"Vrai OK | {cm[0,0]:^10} | {cm[0,1]:^17} |")
        print(f"Vrai D√©f| {cm[1,0]:^10} | {cm[1,1]:^17} |")
        
        # Comparer avec les m√©triques de r√©f√©rence
        if self.metrics:
            print(f"\nüìä Comparaison avec les m√©triques d'entra√Ænement:")
            print(f"    | Entra√Ænement | Test      | Diff√©rence |")
            print(f"    |--------------|-----------|------------|")
            print(f"Accuracy  | {self.metrics['accuracy']:.4f}     | {accuracy:.4f}  | {accuracy-self.metrics['accuracy']:+.4f}    |")
            print(f"Precision | {self.metrics['precision']:.4f}     | {precision:.4f}  | {precision-self.metrics['precision']:+.4f}    |")
            print(f"Recall    | {self.metrics['recall']:.4f}     | {recall:.4f}  | {recall-self.metrics['recall']:+.4f}    |")
            print(f"F1-score  | {self.metrics['f1_score']:.4f}     | {f1:.4f}  | {f1-self.metrics['f1_score']:+.4f}    |")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm
        }

if __name__ == "__main__":
    # Test du syst√®me de d√©ploiement
    qc_system = SemiconductorQualityControl()
    
    # 1. Charger les donn√©es pour le test
    data_path = '../data/secom_preprocessed.csv'
    if os.path.exists(data_path):
        df = pd.read_csv(data_path)
        X = df.drop('Target', axis=1)
        y = df['Target']
        
        # Simuler l'arriv√©e de nouvelles donn√©es (5 premi√®res lignes)
        print("\n--- TEST: Simulation de nouvelles donn√©es ---")
        new_samples = X.head(5)
        predictions, probs = qc_system.predict(new_samples)
        
        # Affichage d√©taill√©
        for i, (pred, prob) in enumerate(zip(predictions, probs)):
            status = "D√âFECTUEUX ‚ùå" if pred == 1 else "OK ‚úÖ"
            print(f"√âchantillon {i+1} : {status} (Probabilit√©: {prob:.4f})")
            
        # 2. √âvaluer les performances globales
        print("\n--- TEST: √âvaluation globale du mod√®le ---")
        qc_system.evaluate_performance(X, y)
    else:
        print(f"‚ùå Erreur: Fichier de donn√©es {data_path} non trouv√©.")
